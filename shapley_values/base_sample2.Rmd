---
title: "TP 2: SHAP"
author: "Damian Fontenla"
date: "29 de Octubre de 2022"
output:
  html_notebook:
    theme: spacelab
    toc: yes
    toc_float: yes
    df_print: paged
---

rm( list=ls() )  #remove all objects
gc()       

library(xgboost)
install.packages("shapr")
library(dplyr)
library(shapr)

library(tidyverse)
library(tidymodels)
library(rsample)
data("Boston", package = "MASS")

print(Boston[, x_var])
print(Boston[1:6, x_var])
print(Boston[-1:-6, x_var])

print(churn_ds[-1:-6,x_var])
print(churn_ds[1:6,x_var])


churn_ds = read.csv("/Users/dfontenla/Maestria/2022C2/EEA/practica/repo/EEA-2022/TP2/WA_Fn-UseC_-Telco-Customer-Churn.csv", encoding = "UTF-8") %>% mutate(id = 1:nrow(.)) 
encuesta_salud_train = read.csv("/Users/dfontenla/Maestria/2022C2/EEA/practica/repo/EEA-2022/TP1/encuesta_salud_train.csv", encoding = "UTF-8") %>% mutate(id = 1:nrow(.)) 

encuesta_salud_train_gender_numeric = encuesta_salud_train %>%
                              dplyr::select(edad, genero, altura, peso,dias_consumo_comida_rapida, consumo_diario_alcohol, 
                              dias_actividad_fisica_semanal)

encuesta_salud_train_gender_numeric                              
summary(churn_ds)

#x_var <- c("id","customerID", "gender", "SeniorCitizen", "Partner","Dependents", "tenure", "PhoneService", "MultipleLines","InternetService", "OnlineSecurity", "OnlineBackup", "DeviceProtection",TechSupport", "StreamingTV", "StreamingMovies", "Contract",PaperlessBilling", "PaymentMethod", "MonthlyCharges", "TotalCharges")
x_var <- c( "TotalCharges")
#, "PhoneService", "MultipleLines","InternetService", "OnlineSecurity", "OnlineBackup", "DeviceProtection","TechSupport", "StreamingTV", "StreamingMovies", "Contract","PaperlessBilling", "PaymentMethod", "MonthlyCharges", "TotalCharges"
y_var <- "Churn"


x_var <- c( "edad","genero","altura","dias_consumo_comida_rapida","consumo_diario_alcohol")
y_var <- "peso"

colnames(churn_ds)

x_train <- as.matrix(encuesta_salud_train_gender_numeric[-1:-6,x_var])
y_train <- encuesta_salud_train_gender_numeric[-1:-6, y_var]
x_test <- as.matrix(encuesta_salud_train_gender_numeric[1:6, x_var])

data_x <- as.data.frame(x_train)
data_x$genero = as.factor(data_x$genero)
data_x$genero <- as.numeric(as.character(data_x$genero))  # Convert one variable to numeric
x_train <- as.matrix(data_x)
x_train


x_train <- as.matrix(data_x)
x_train <- unclass(x_train)
x_train
data_x <- unclass(data_x)
data_x
data_new <- sapply(data_x, unclass)           # Convert categorical variables
data_new[,"genero"]
unclass(x)
Boston$rad = as.factor(Boston$rad)
data_x$genero2 <- as.numeric(data_x$genero2)  # Convert one variable to numeric
data_x$genero2
summary(data_x)
colnames(x_train)
churn_ds[,"OnlineBackup"]
# Fitting a basic xgboost model to the training data
install.packages("SparseM")
library("SparseM")

install.packages("Matrix")
library("Matrix")

require(xgboost)
require(Matrix)
require(data.table)
if (!require('vcd')) install.packages('vcd')

sparse_matrix <- sparse.model.matrix(y_train ~ ., data = churn_ds)[,-1]
X_train_dmat = xgb.DMatrix(sparse_matrix, label = churn_ds$y_train)

infinitos      <- lapply(names(x_train),function(.name) x_train[ , sum(is.infinite(get(.name)))])
infinitos_qty  <- sum( unlist( infinitos) )
if( infinitos_qty > 0 )
{
  cat( "ATENCION, hay", infinitos_qty, "valores infinitos en tu dataset. Seran pasados a NA\n" )
  x_train[mapply(is.infinite, dataset)] <- NA
}


#valvula de seguridad para evitar valores NaN  que es 0/0
#paso los NaN a 0 , decision polemica si las hay
#se invita a asignar un valor razonable segun la semantica del campo creado
nans      <- lapply(names(x_train),function(.name) x_train[ , sum(is.nan(get(.name)))])
nans_qty  <- sum( unlist( nans) )
if( nans_qty > 0 )
{
  cat( "ATENCION, hay", nans_qty, "valores NaN 0/0 en tu dataset. Seran pasados arbitrariamente a 0\n" )
  cat( "Si no te gusta la decision, modifica a gusto el programa!\n\n")
  x_train[mapply(is.nan, x_train)] <- 0
}

x_train
model <- xgboost(
  data = x_train,
  label = y_train,
  nround = 20,
  verbose = FALSE
)

# Prepare the data for explanation
explainer <- shapr(x_train, model)
#> The specified model provides feature classes that are NA. The classes of data are taken as the truth.

# Specifying the phi_0, i.e. the expected prediction without any features
p <- mean(y_train)

# Computing the actual Shapley values with kernelSHAP accounting for feature dependence using
# the empirical (conditional) distribution approach with bandwidth parameter sigma = 0.1 (default)
explanation <- explain(
  x_test,
  approach = "empirical",
  explainer = explainer,
  prediction_zero = p
)

# Printing the Shapley values for the test data.
# For more information about the interpretation of the values in the table, see ?shapr::explain.
print(explanation$dt)
#>      none     lstat         rm       dis      indus
#> 1: 22.446 5.2632030 -1.2526613 0.2920444  4.5528644
#> 2: 22.446 0.1671903 -0.7088405 0.9689007  0.3786871
#> 3: 22.446 5.9888016  5.5450861 0.5660136 -1.4304350
#> 4: 22.446 8.2142203  0.7507569 0.1893368  1.8298305
#> 5: 22.446 0.5059890  5.6875106 0.8432240  2.2471152
#> 6: 22.446 1.9929674 -3.6001959 0.8601984  3.1510530

# Plot the resulting explanations for observations 1 and 6
plot(explanation, plot_phi0 = FALSE, index_x_test = c(1, 6))


# Use the Gaussian approach
explanation_gaussian <- explain(
  x_test,
  approach = "gaussian",
  explainer = explainer,
  prediction_zero = p
)

# Plot the resulting explanations for observations 1 and 6
plot(explanation_gaussian, plot_phi0 = FALSE, index_x_test = c(1, 6))


# Use the Gaussian copula approach
explanation_copula <- explain(
  x_test,
  approach = "copula",
  explainer = explainer,
  prediction_zero = p
)

# Plot the resulting explanations for observations 1 and 6, excluding
# the no-covariate effect
plot(explanation_copula, plot_phi0 = FALSE, index_x_test = c(1, 6))

install.packages("partykit")
library("partykit")
# Use the conditional inference tree approach
explanation_ctree <- explain(
  x_test,
  approach = "ctree",
  explainer = explainer,
  prediction_zero = p
)

# Plot the resulting explanations for observations 1 and 6, excluding 
# the no-covariate effect
plot(explanation_ctree, plot_phi0 = FALSE, index_x_test = c(1, 6))

# We can use mixed (i.e continuous, categorical, ordinal) data with ctree. Use ctree with categorical data in the following manner:

x_var_cat <- c("lstat", "chas", "rad", "indus")
y_var <- "medv"

# convert to factors
Boston$rad = as.factor(Boston$rad)
Boston$chas = as.factor(Boston$chas)

x_train_cat <- Boston[-1:-6, x_var_cat]
y_train <- Boston[-1:-6, y_var]
x_test_cat <- Boston[1:6, x_var_cat]

# -- special function when using categorical data + xgboost
dummylist <- make_dummies(traindata = x_train_cat, testdata = x_test_cat)

x_train_dummy <- dummylist$train_dummies
x_test_dummy <- dummylist$test_dummies

# Fitting a basic xgboost model to the training data
model_cat <- xgboost::xgboost(
  data = x_train_dummy,
  label = y_train,
  nround = 20,
  verbose = FALSE
)
model_cat$feature_list <- dummylist$feature_list

explainer_cat <- shapr(dummylist$traindata_new, model_cat)

p <- mean(y_train)

explanation_cat <- explain(
  dummylist$testdata_new,
  approach = "ctree",
  explainer = explainer_cat,
  prediction_zero = p
)

# Plot the resulting explanations for observations 1 and 6, excluding
# the no-covariate effect
plot(explanation_cat, plot_phi0 = FALSE, index_x_test = c(1, 6))

# Use the conditional inference tree approach
# We can specify parameters used to building trees by specifying mincriterion, 
# minsplit, minbucket

explanation_ctree <- explain(
  x_test,
  approach = "ctree",
  explainer = explainer,
  prediction_zero = p,
  mincriterion = 0.80, 
  minsplit = 20,
  minbucket = 20
)

# Default parameters (based on (Hothorn, 2006)) are:
# mincriterion = 0.95
# minsplit = 20
# minbucket = 7

# Use the conditional inference tree approach
# Specify a vector of mincriterions instead of just one
# In this case, when conditioning on 1 or 2 features, use mincriterion = 0.25
# When conditioning on 3 or 4 features, use mincriterion = 0.95

explanation_ctree <- explain(
  x_test,
  approach = "ctree",
  explainer = explainer,
  prediction_zero = p,
  mincriterion = c(0.25, 0.25, 0.95, 0.95)
)



#### shapr currently natively supports explanation of predictions from models fitted with the following functions:

install.packages("gbm")
library(gbm)
#> Loaded gbm 2.1.5

xy_train <- data.frame(x_train,medv = y_train)

form <- as.formula(paste0(y_var,"~",paste0(x_var,collapse="+")))

# Fitting a gbm model
set.seed(825)
model <- gbm::gbm(
  form,
  data = xy_train,
  distribution = "gaussian"
)

#### Full feature versions of the three required model functions ####

predict_model.gbm <- function(x, newdata) {
  
  if (!requireNamespace('gbm', quietly = TRUE)) {
    stop('The gbm package is required for predicting train models')
  }

  model_type <- ifelse(
    x$distribution$name %in% c("bernoulli","adaboost"),
    "classification",
    "regression"
  )
  if (model_type == "classification") {

    predict(x, as.data.frame(newdata), type = "response",n.trees = x$n.trees)
  } else {

    predict(x, as.data.frame(newdata),n.trees = x$n.trees)
  }
}

get_model_specs.gbm <- function(x){
  feature_list = list()
  feature_list$labels <- labels(x$Terms)
  m <- length(feature_list$labels)

  feature_list$classes <- attr(x$Terms,"dataClasses")[-1]
  feature_list$factor_levels <- setNames(vector("list", m), feature_list$labels)
  feature_list$factor_levels[feature_list$classes=="factor"] <- NA # the model object doesn't contain factor levels info

  return(feature_list)
}

# Prepare the data for explanation
set.seed(123)
explainer <- shapr(xy_train, model)
#> The columns(s) medv is not used by the model and thus removed from the data.
p0 <- mean(xy_train[,y_var])
explanation <- explain(x_test, explainer, approach = "empirical", prediction_zero = p0)
# Plot results
plot(explanation)

